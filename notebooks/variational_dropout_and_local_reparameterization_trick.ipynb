{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pyro\n",
    "import pyro.contrib.bnn as bnn\n",
    "import pyro.optim\n",
    "import torch.nn.functional as F\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "from torch.distributions import constraints\n",
    "from pyro import poutine\n",
    "import pyro.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNeuralRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    See variational dropout and the local reparameterization trick paper.\n",
    "    The pre-activations are sampled directly:\n",
    "        z ~ N(Xμ, Xσ²)\n",
    "    For a hidden layer of 1000 in, 1000 out.\n",
    "    This reduces sampling 1M parameters to 1000 pre-activations.\n",
    "\n",
    "    This also leads to Gaussian dropout.\n",
    "    Gaussian dropout; The posterior of the weights\n",
    "\n",
    "    q(w) = N(Φ, αΦ²) where α = p / (1 - p)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def model(self, x, y, kl_factor=1):\n",
    "        z1_mean = torch.zeros(self.input_size, self.hidden_size)\n",
    "        z1_scale = torch.ones(self.input_size, self.hidden_size)\n",
    "        z1_dropout = torch.tensor(gaussian_dropout_alpha(self.dropout))\n",
    "\n",
    "        # Note the extra column is the bias. Is automatically added.\n",
    "        z2_mean = torch.zeros(self.hidden_size + 1, self.hidden_size)\n",
    "        z2_scale = torch.ones(self.hidden_size + 1, self.hidden_size)\n",
    "        z2_dropout = torch.tensor(gaussian_dropout_alpha(self.dropout))\n",
    "\n",
    "        z3_mean = torch.zeros(self.hidden_size + 1, self.output_size)\n",
    "        z3_scale = torch.ones(self.hidden_size + 1, self.output_size)\n",
    "        z3_dropout = torch.tensor(gaussian_dropout_alpha(0.2))\n",
    "\n",
    "        with pyro.plate('data', size=x.shape[0]):\n",
    "            a = pyro.sample('a1',\n",
    "                             bnn.HiddenLayer(x, z1_mean, z1_dropout * z1_scale,\n",
    "                                             non_linearity=F.leaky_relu,\n",
    "                                             KL_factor=kl_factor)\n",
    "                             )\n",
    "            a = pyro.sample('a2',\n",
    "                             bnn.HiddenLayer(a, z2_mean, z2_dropout * z2_scale,\n",
    "                                             non_linearity=F.leaky_relu,\n",
    "                                             KL_factor=kl_factor)\n",
    "                             )\n",
    "            a = pyro.sample('last_activation',\n",
    "                             bnn.HiddenLayer(a, z3_mean, z3_dropout * z3_scale,\n",
    "                                             non_linearity=F.leaky_relu,\n",
    "                                             KL_factor=kl_factor,\n",
    "                                             include_hidden_bias=False)\n",
    "                             )\n",
    "            sigma = pyro.sample('sigma', dist.HalfNormal(scale=torch.tensor(1.)))\n",
    "            return pyro.sample('obs', dist.Normal(loc=a, scale=sigma), obs=y)\n",
    "\n",
    "    def guide(self, x, y=None, kl_factor=1):\n",
    "        z1_mean = pyro.param('z1_mean', 0.01 * torch.randn(self.input_size, self.hidden_size))\n",
    "        z1_scale = pyro.param('z1_scale', 0.1 * torch.ones(self.input_size, self.hidden_size),\n",
    "                              constraint=constraints.greater_than(0.01))\n",
    "        z1_dropout = pyro.param('z1_dropout', torch.tensor(self.dropout),\n",
    "                                constraint=constraints.interval(0.1, 1.0))\n",
    "\n",
    "        z2_mean = pyro.param('z2_mean', 0.01 * torch.randn(self.hidden_size + 1, self.hidden_size))\n",
    "        z2_scale = pyro.param('z2_scale', 0.1 * torch.ones(self.hidden_size + 1, self.hidden_size),\n",
    "                              constraint=constraints.greater_than(0.01))\n",
    "        z2_dropout = pyro.param('z2_dropout', torch.tensor(self.dropout),\n",
    "                                constraint=constraints.interval(0.1, 1.0))\n",
    "\n",
    "        z3_mean = pyro.param('z3_mean', 0.01 * torch.randn(self.hidden_size + 1, self.output_size))\n",
    "        z3_scale = pyro.param('z3_scale', 0.1 * torch.ones(self.hidden_size + 1, self.output_size),\n",
    "                              constraint=constraints.greater_than(0.01))\n",
    "        z3_dropout = pyro.param('z3_dropout', torch.tensor(self.dropout),\n",
    "                                constraint=constraints.interval(0.1, 1.0))\n",
    "        sigma_scale = pyro.param('sigma_scale', torch.tensor(1.), constraint=constraints.interval(0.01, 2.))\n",
    "\n",
    "        with pyro.plate('data', size=x.shape[0]):\n",
    "            a = pyro.sample('a1',\n",
    "                             bnn.HiddenLayer(x, z1_mean, z1_dropout * z1_scale,\n",
    "                                             non_linearity=F.leaky_relu,\n",
    "                                             KL_factor=kl_factor)\n",
    "                             )\n",
    "            a = pyro.sample('a2',\n",
    "                             bnn.HiddenLayer(a, z2_mean, z2_dropout * z2_scale,\n",
    "                                             non_linearity=F.leaky_relu,\n",
    "                                             KL_factor=kl_factor)\n",
    "                             )\n",
    "            a = pyro.sample('last_activation',\n",
    "                             bnn.HiddenLayer(a, z3_mean, z3_dropout * z3_scale,\n",
    "                                             non_linearity=F.leaky_relu,\n",
    "                                             KL_factor=kl_factor,\n",
    "                                             include_hidden_bias=False\n",
    "                                             )\n",
    "                             )\n",
    "            pyro.sample('sigma', dist.HalfNormal(scale=sigma_scale))\n",
    "\n",
    "    def fit(self, x_train, y_train, lr=0.01, epochs=30, batch_size=1024):\n",
    "        optim = pyro.optim.Adam({'lr': lr})\n",
    "        elbo = TraceMeanField_ELBO()\n",
    "        svi = SVI(self.model, self.guide, optim, elbo)\n",
    "        kl_factor = batch_size / x_train.shape[0]\n",
    "\n",
    "        train_idx = np.arange(y_train.shape[0])\n",
    "        max_idx_train = train_idx.shape[0]\n",
    "        global_step = 0\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            np.random.shuffle(train_idx)\n",
    "            c = 0\n",
    "            print('\\nEpoch:', e)\n",
    "            while c < max_idx_train - batch_size:\n",
    "                selection = train_idx[c: c + batch_size]\n",
    "                x = x_train[selection]\n",
    "                y = y_train[selection]\n",
    "\n",
    "                loss = svi.step(x, y, kl_factor=kl_factor)\n",
    "\n",
    "                if c % (100) == 0:\n",
    "                    loss /= len(selection)\n",
    "                    losses.append(loss)\n",
    "                    print(\"[iteration {:6}] loss: {:.2f}\".format(c + 1, loss))\n",
    "\n",
    "                c += batch_size\n",
    "                global_step += batch_size\n",
    "        return losses\n",
    "\n",
    "    def forward(self, x, n_samples=10):\n",
    "        with torch.no_grad():\n",
    "            res = []\n",
    "            for i in range(n_samples):\n",
    "                t = poutine.trace(self.guide).get_trace(x, None)\n",
    "                res.append(t.nodes['last_activation']['value'])\n",
    "            return torch.stack(res, dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
